\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\theoremstyle{definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\usepackage{multirow}

%--

%--
\begin{document}
\title{机器学习导论\\
习题六}
\author{141242006, 袁帅, 141242006@smail.nju.edu.cn}
\maketitle

\section{[20pts] Ensemble Methods}
\begin{enumerate}[ {(}1{)}]
\item \textbf{[10pts]} 试说明Boosting的核心思想是什么，Boosting中什么操作使得基分类器具备多样性？

\item \textbf{[10pts]} 试析随机森林为何比决策树Bagging集成的训练速度更快。
\end{enumerate}
\begin{solution}
\item[(1)] The key idea in Boosting is to distinguish sample weights, attaching more importance to mistaken samples and less importance to correctly classified samples. It combines all learners' results as the output. The diversity in Boosting is provided by the different datasets or weights used in the training session.
\item[(2)] In every iteration, Random Forests randomly pick a small feature set and select an optimal feature out of that set to split, so the cost of splitting would be decreased dramatically. In addition, in Random Forest, we don't require every decision tree to be completely precise, so we can set arguments such as maximum samples per leaf or maximum depth to restrain the complexity of each decision tree, so the training process will be faster.
~\\
\end{solution}

\section{[20pts] Bagging}
考虑一个回归学习任务$f:\mathbb{R}^d \rightarrow \mathbb{R}$。假设我们已经学得$M$个学习器$\hat{f}_1(\mathbf{x}),\hat{f}_2(\mathbf{x}),\dots,\hat{f}_M(\mathbf{x})$。我们可以将学习器的预测值看作真实值项加上误差项
\begin{equation}
\hat{f}_m(\mathbf{x})=f(\mathbf{x})+\epsilon_m(\mathbf{x})
\end{equation}
每个学习器的期望平方误差为$\mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})^2]$。所有的学习器的期望平方误差的平均值为
\begin{equation}
E_{av}=\frac{1}{M}\sum_{m=1}^M \mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})^2]
\end{equation}
M个学习器得到的Bagging模型为
\begin{equation}
\hat{f}_{bag}(\mathbf{x})=\frac{1}{M}\sum_{m=1}^M \hat{f}_m(\mathbf{x})
\end{equation}
Bagging模型的误差为
\begin{equation}
\epsilon_{bag}(\mathbf{x})=\hat{f}_{bag}(\mathbf{x})-f(\mathbf{x})=\frac{1}{M}\sum_{m=1}^M \epsilon_m(\mathbf{x})
\end{equation}
其期望平均误差为
\begin{equation}
E_{bag}=\mathbb{E}_{\mathbf{x}}[\epsilon_{bag}(\mathbf{x})^2]
\end{equation}
\begin{enumerate}[ {(}1{)}]
\item \textbf{[10pts]} 假设$\forall\; m\neq l$，$ \mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})]=0$，$ \mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})\epsilon_l(\mathbf{x})]=0$。证明
\begin{equation}
E_{bag}=\frac{1}{M} E_{av}
\end{equation}

\item  \textbf{[10pts]} 试证明不需对$\epsilon_m(\mathbf{x})$做任何假设，$E_{bag}\leq E_{av}$始终成立。（提示：使用Jensen's inequality）
\end{enumerate}

\begin{prove}
\item[(1)]
\begin{eqnarray}
E_{\text{bag}}&=&\mathbb{E}_{\bm{x}}[\epsilon_{\text{bag}}(\bm{x})^2]=\mathbb{E}_{\bm{x}}[\frac{1}{M^2}\sum_{m=1}^M\epsilon_m(\bm{x})\sum_{l=1}^M\epsilon_l(\bm{x})]=\frac{1}{M^2}\sum_{m=1}^M\sum_{l=1}^M\mathbb{E}_{\bm{x}}[\epsilon_m(\bm{x})\epsilon_l(\bm{x})]\nonumber\\
&=&\frac{1}{M^2}\sum_{m=1}^M\mathbb{E}_{\bm{x}}[\epsilon_m(\bm{x})^2]=\frac{1}{M}E_{\text{av}}.\nonumber
\end{eqnarray}

\item[(2)] By Cauchy's Inequality, we obtain $\forall \bm{x$}, 
\begin{equation}
\left(\sum_{m=1}^M\epsilon_m(\bm{x})\right)^2\leq\left(\sum_{m=1}^M1^2\right)\left(\sum_{m=1}^M\epsilon_m(\bm{x})^2\right)=M\sum_{m=1}^M\epsilon_m(\bm{x})^2,\nonumber
\end{equation}
which gives $\mathbb{E}_{\bm{x}}[\left(\sum_{m=1}^M\epsilon_m(\bm{x})\right)^2]\leq M\cdot\mathbb{E}_{\bm{x}}[\sum_{m=1}^M\epsilon_m(\bm{x})^2].$
Therefore, we have
\begin{equation}
E_{\text{bag}}=\mathbb{E}_{\bm{x}}[\epsilon_{\text{bag}}(\bm{x})^2]=\frac{1}{M^2}\mathbb{E}_{\bm{x}}[\left(\sum_{m=1}^M\epsilon_m(\bm{x})\right)^2]\leq\frac{1}{M}\mathbb{E}_{\bm{x}}[\sum_{m=1}^M\epsilon_m(\bm{x})^2]=\frac{1}{M}\sum_{m=1}^M\mathbb{E}_{\bm{x}}[\epsilon_m(\bm{x})^2]=E_{\text{av}}.\nonumber
\end{equation}.
~\\
\qed
\end{prove}


\section{[30pts] AdaBoost in Practice}

\begin{enumerate}[ {(}1{)}]
\item \textbf{[25pts]} 请实现以Logistic Regression为基分类器的AdaBoost，观察不同数量的ensemble带来的影响。详细编程题指南请参见链接：
\url{http://lamda.nju.edu.cn/ml2017/PS6/ML6_programming.html}
\item \textbf{[5pts]} 在完成上述实践任务之后，你对AdaBoost算法有什么新的认识吗？请简要谈谈。
\end{enumerate}
\begin{solution}
I wrote two versions of AdaBoost, namely \textit{AdaBoost\_resample()} and \textit{AdaBoost\_reweight()}, in regards to the different ways of utilizing sample weights. In both cases, I use the default L2-regularized Logistic Regression as base classifier. The results are listed below: 

\begin{table}[h!]
\centering
\begin{tabular}{l|l|cccc}
\multirow{2}{*}{}              & \multirow{2}{*}{Total running time}  & \multicolumn{4}{l}{\quad\quad\quad\quad\quad\quad Accuracy}             \\
                               &                                      & 1 base  & 5 bases & 10 bases & 100 bases \\ \hline
\multicolumn{1}{c|}{Re-weight(C=1)} & \multicolumn{1}{c|}{< 1 second}  & 57.52\% & 72.70\% & 73.97\%  & 73.97\% \\
\multicolumn{1}{c|}{Re-sample(C=1)} & \multicolumn{1}{c|}{6$\sim$8 seconds} & 80.68\% & 82.58\% & 84.46\%  & 84.05\%   \\
\multicolumn{1}{c|}{Re-sample(C=100)} & \multicolumn{1}{c|}{50 seconds} & 81.95\% & 84.88\% & 85.08\%  & 87.60\%
\end{tabular}
\end{table}
There is one pitfall in this assignment: the labels are 0s and 1s; however, the AdaBoost Algorithm on textbook only applies for labels \{-1, +1\} because there is a weighted summation process as the output. Generally speaking, the accuracy is increasing when we use more base learners, so Ensemble Learning does help! Changing parameters could make a significant difference because all base learners' performance are altered and the degree of diversity is also changed.

~\\
\end{solution}
\end{document}