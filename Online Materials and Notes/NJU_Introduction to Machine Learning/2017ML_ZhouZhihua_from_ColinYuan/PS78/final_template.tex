\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsfonts, amsmath, amsthm, bm, amssymb}
\numberwithin{equation}{section}
\usepackage[ruled,vlined,lined,boxed,linesnumbered]{algorithm2e}
\theoremstyle{definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\renewcommand\proofname{Proof}
\renewcommand\refname{Reference}
\newtheorem*{Remark}{Remark}

%--

%--
\begin{document}
\title{机器学习导论\\
综合能力测试}
\author{141242006, 袁帅, 141242006@smail.nju.edu.cn}
\maketitle
\section{[40pts] Exponential Families}
\label{Exponential Families}
指数分布族(\href{https://en.wikipedia.org/wiki/Exponential_family}{Exponential Families})是一类在机器学习和统计中非常常见的分布族, 具有良好的性质。在后文不引起歧义的情况下, 简称为指数族。

指数分布族是一组具有如下形式概率密度函数的分布族群:
\begin{equation}
f_X(x|\theta) = h(x) \exp \left(\eta(\theta) \cdot T(x) -A(\theta)\right)
\end{equation}  
其中, $\eta(\theta)$, $A(\theta)$以及函数$T(\cdot)$, $h(\cdot)$都是已知的。
\begin{enumerate}[(1)]
\item \textbf{[10pts]} 试证明多项分布(\href{https://en.wikipedia.org/wiki/Multinomial_distribution}{Multinomial distribution})属于指数分布族。

\item \textbf{[10pts]} 试证明多元高斯分布(\href{https://en.wikipedia.org/wiki/Multivariate_normal_distribution}{Multivariate Gaussian distribution})属于指数分布族。

\item \textbf{[20pts]} 考虑样本集$\mathcal{D}=\{ x_1,\cdots, x_n\}$是从某个已知的指数族分布中独立同分布地(i.i.d.)采样得到, 即对于$\forall i\in [1,n]$, 我们有$f( x_i|\boldsymbol\theta) = h(x_i) \exp \left ( {\boldsymbol\theta}^{\rm T}T(x_i) -A(\boldsymbol\theta)\right)$. 

对参数$\boldsymbol\theta$, 假设其服从如下先验分布：
\begin{equation}
p_\pi(\boldsymbol\theta|\boldsymbol\chi,\nu) = f(\boldsymbol\chi,\nu) \exp \left (\boldsymbol\theta^{\rm T} \boldsymbol\chi - \nu A(\boldsymbol\theta) \right )
\end{equation}
其中, $\boldsymbol\chi$和$\nu$是$\boldsymbol\theta$生成模型的参数。请计算其后验, 并证明后验与先验具有相同的形式。(\textbf{Hint}: 上述又称为“共轭”(\href{https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf}{Conjugacy}),在贝叶斯建模中经常用到)
\end{enumerate}


\begin{solution}
\item[(1)]
\begin{proof}
The Multinomial Distribution's probability mass function can be rewritten as
\begin{eqnarray}
P(\bm{x}|n, p_1, p_2,\cdots, p_k)&=&\frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots p_k^{x_k}\nonumber\\
&=&\frac{n!}{x_1!x_2!\cdots x_k!}\exp(x_1\ln p_1+x_2\ln p_2+\cdots+x_k\ln p_k)\nonumber\\
&=&h(\bm{x})\exp(\bm{\eta}^\mathrm{T}{(\bm{\theta}})\cdot \bm{T}(\bm{X})-A(\bm{\theta})),
\end{eqnarray}
in which $\bm{\theta}=\{n, p_1,p_2, \dots, p_k\}$, $h(\bm{x})=\frac{n!}{x_1!x_2!\cdots x_k!}$, $\bm{\eta}(\bm{\theta})=\left[\begin{array}{c}\ln p_1\\\ln p_2\\\vdots\\\ln p_k\end{array}\right]$, $\bm{T}(\bm{x})=\left[\begin{array}{c}x_1\\x_2\\\vdots\\x_k\end{array}\right]$, $A(\bm{\theta})=0$, and the multiplication 
$\bm{\eta}^\mathrm{T}(\bm{\theta})\cdot \bm{T}(\bm{X})$ is the dot product of two vectors.
\end{proof}

\item[(2)]
\begin{proof}
The Multivariate Gaussian Distribution's probability density function is
\begin{eqnarray}
p(\bm{x}|\bm{\mu}, \bm{\Sigma})&=&\frac{1}{(2\pi)^{d/2}|\bm{\Sigma|}^{1/2}}\exp[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu})]\nonumber\\
&=&\exp[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu})-\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln|\bm{\Sigma}|]\nonumber\\
&=&\exp[-\frac{1}{2}\bm{x}^\mathrm{T}\bm{\Sigma}^{-1}\bm{x}+\bm{\mu}^\mathrm{T}\bm{\Sigma}^{-1}\bm{x}-\frac{1}{2}\bm{\mu}^\mathrm{T}\bm{\Sigma}^{-1}\bm{\mu}-\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln|\bm{\Sigma}|]\nonumber\\
&=&\exp[-\frac{1}{2}\sum_{i=1}^d\sum_{j=1}^d(\bm{\Sigma}^{-1})_{ij}x_ix_j+\sum_{i=1}^d(\bm{\mu}^\mathrm{T}\bm{\Sigma}^{-1})_{i}x_i-A(\bm{\theta})]\nonumber\\
&=&\exp[-\frac{1}{2}\text{Vec}^\mathrm{T}(\bm{\Sigma}^{-1})\cdot\text{Vec}(\bm{x}\bm{x}^\mathrm{T})+(\bm{\Sigma}^{-1}\bm{\mu})^\mathrm{T}\cdot\bm{x}-A(\bm{\theta})]\nonumber\\
&=&h(\bm{x})\exp(\bm{\eta}^\mathrm{T}{(\bm{\theta}})\cdot \bm{T}(\bm{X})-A(\bm{\theta})),
\end{eqnarray}
in which $\text{Vec}(\cdot)$ is the vectorization operator, $\bm{\theta}=\{\bm{\mu}, \bm{\Sigma}\}$, $h(\bm{x})=1$, $\bm{\eta}(\bm{\theta})=\left[\begin{array}{c}-\frac{1}{2}\text{Vec}(\bm{\Sigma}^{-1})\\\bm{\Sigma}^{-1}\bm{\mu}\end{array}\right]$, $\bm{T}(\bm{x})=\left[\begin{array}{c}\text{Vec}(\bm{x}\bm{x}^{\mathrm{T}})\\\bm{x}\end{array}\right]$, $A(\bm{\theta})=\frac{1}{2}\bm{\mu}^\mathrm{T}\bm{\Sigma}^{-1}\bm{\mu}+\frac{d}{2}\ln(2\pi)+\frac{1}{2}\ln|\bm{\Sigma}|$, and the multiplication 
$\bm{\eta}^\mathrm{T}(\bm{\theta})\cdot \bm{T}(\bm{X})$ is the dot product of two vectors.
\end{proof}

\item[(3)] According to Wikipedia\cite{ref: conj_dist}, The posterior distribution could be given as
\begin{eqnarray}
p(\bm{\theta}|\mathcal{D}; \bm{\chi}, {\nu})&=&\frac{p(\bm{\theta}|\bm{\chi},\nu)p(\mathcal{D}|\bm{\theta}; \bm{\chi}, {\nu})}{p(\mathcal{D}|\bm\chi, \nu)}\nonumber\\
&=&\frac{f(\bm\chi, \nu)\exp(\bm\theta^\mathrm{T}\bm\chi-\nu A(\bm\theta))\prod_{i=1}^nh(\bm{x}_i)\exp(\bm\theta^\mathrm{T}\bm{T}(\bm{x}_i)-A(\bm{\theta}))}{\int_{\bm\alpha} f(\bm\chi, \nu)\exp(\bm\alpha^\mathrm{T}\bm\chi-\nu A(\bm\alpha))\prod_{i=1}^nh(\bm{x}_i)\exp(\bm\alpha^\mathrm{T}\bm{T}(\bm{x}_i)-A(\bm{\alpha}))}\nonumber\\
&=&\frac{\exp[\bm\theta^\mathrm{T}(\bm\chi+\sum_{i=1}^n\bm{T}(\bm{x}_i))-(\nu+n)A(\bm\theta)]}{\int_{\bm\alpha}{\exp[\bm\alpha^\mathrm{T}(\bm\chi+\sum_{i=1}^n\bm{T}(\bm{x}_i))-(\nu+n)A(\bm\alpha)]}}\nonumber\\
&=&\hat{f}(\bm{\hat{\chi}}, \hat\nu)\exp(\bm\theta^\mathrm{T}\hat{\bm\chi}-\hat\nu \hat A(\bm\theta)),
\end{eqnarray}
where $\hat{\bm\chi}=\bm\chi+\sum_{i=1}^n\bm{T}(\bm{x}_i)$, $\hat\nu=\nu+n$, $\hat A(\bm\theta)=A(\bm\theta)$, $\hat{f}(\bm{\hat{\chi}}, \hat\nu)=\frac{1}{\int_{\bm\alpha}{\exp[\bm\alpha^\mathrm{T}\hat{\bm\chi}-\hat\nu A(\bm\alpha)]}}$. Therefore, the posterior distribution is of the same form as the prior.
~\\
\end{solution}

\section{[40pts] Decision Boundary}
考虑二分类问题, 特征空间$X \in \mathcal{X}= \mathbb{R}^d$, 标记$Y \in \mathcal{Y}= \{0, 1\}$. 我们对模型做如下生成式假设：
\begin{itemize}
\item[-] attribute conditional independence assumption: 对已知类别, 假设所有属性相互独立, 即每个属性特征独立地对分类结果发生影响；
\item[-] Bernoulli prior on label: 假设标记满足Bernoulli分布先验, 并记$\Pr(Y=1) = \pi$. 
\end{itemize}

\begin{enumerate}[(1)]
\item \textbf{[20pts]} 假设$P(X_i | Y)$服从指数族分布, 即
\[
\Pr(X_i = x_i | Y = y) = h_i(x_i) \exp (\theta_{iy} \cdot T_i(x_i) - A_{i}(\theta_{iy}))
\]
请计算后验概率分布$\Pr(Y | X)$以及分类边界$\{x \in \mathcal{X}: P(Y=1 | X = x) = P(Y=0 | X =x)\}$. (\textbf{Hint}: 你可以使用sigmoid函数$\mathcal{S}(x)=1/(1+e^{-x})$进行化简最终的结果).

\item \textbf{[20pts]} 假设$P(X_i | Y=y)$服从高斯分布, 且记均值为$\mu_{iy}$以及方差为$\sigma_{i}^2$ (注意, 这里的方差与标记$Y$是独立的), 请证明分类边界与特征$X$是成线性的。 
\end{enumerate}
\begin{solution}
\item[(1)] The posterior distribution is given by
\begin{eqnarray}
P(Y=0|\bm{X}=\bm{x})&=&\frac{P(Y=0)P(\bm{X}=\bm{x}|Y=0)}{P(\bm{X}=\bm{x})}\nonumber\\
&=&\frac{(1-\pi)\prod_{i=1}^d\exp(\bm\theta_{i0}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i0}))}{(1-\pi)\prod_{i=1}^d\exp(\bm\theta_{i0}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i0}))+\pi\prod_{i=1}^d\exp(\bm\theta_{i1}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i1}))}\nonumber\\
&=&\frac{1}{1+\frac{\pi}{1-\pi}\exp(\sum_{i=1}^d(\bm\theta_{i1}-\bm\theta_{i0})^\mathrm{T}\cdot\bm{T}_i(x_i)-\sum_{i=1}^dA_i(\bm{\theta}_{i1})+\sum_{i=1}^dA_i(\bm{\theta}_{i0}))}\nonumber\\
&=&\mathcal{S}(\sum_{i=1}^d(\bm\theta_{i0}-\bm\theta_{i1})^\mathrm{T}\bm{T}_i(x_i)-\sum_{i=1}^dA_i(\bm{\theta}_{i0})+\sum_{i=1}^dA_i(\bm{\theta}_{i1})+\ln\frac{1-\pi}{\pi}),
\end{eqnarray}
\begin{eqnarray}
P(Y=1|\bm{X}=\bm{x})&=&\frac{P(Y=1)P(\bm{X}=\bm{x}|Y=1)}{P(\bm{X}=\bm{x})}\nonumber\\
&=&\frac{\pi\prod_{i=1}^d\exp(\bm\theta_{i1}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i1}))}{(1-\pi)\prod_{i=1}^d\exp(\bm\theta_{i0}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i0}))+\pi\prod_{i=1}^d\exp(\bm\theta_{i1}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i1}))}\nonumber\\
&=&\frac{1}{1+\frac{1-\pi}{\pi}\exp(\sum_{i=1}^d(\bm\theta_{i0}-\bm\theta_{i1})^\mathrm{T}\cdot\bm{T}_i(x_i)+\sum_{i=1}^dA_i(\bm{\theta}_{i1})-\sum_{i=1}^dA_i(\bm{\theta}_{i0}))}\nonumber\\
&=&\mathcal{S}(\sum_{i=1}^d(\bm\theta_{i1}-\bm\theta_{i0})^\mathrm{T}\bm{T}_i(x_i)+\sum_{i=1}^dA_i(\bm{\theta}_{i0})-\sum_{i=1}^dA_i(\bm{\theta}_{i1})-\ln\frac{1-\pi}{\pi}).
\end{eqnarray}
The decision boundary is determined by $P(Y=0|\bm{X}=\bm{x})=P(Y=1|\bm{X}=\bm{x})$, which yields
\begin{equation}
P(Y=0)P(\bm{X}=\bm{x}|Y=0)=P(Y=1)P(\bm{X}=\bm{x}|Y=1),
\end{equation}
i.e.
\begin{equation}
\centering
(1-\pi)\prod_{i=1}^d\exp(\bm\theta_{i0}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i0}))=\pi\prod_{i=1}^d\exp(\bm\theta_{i1}^\mathrm{T}\cdot\bm{T}_i(x_i)-A_i(\bm{\theta}_{i1})).
\end{equation}
Solving that equation, we finally obtain the boundary as
\begin{equation}
\sum_{i=1}^d(\bm\theta_{i1}-\bm\theta_{i0})^\mathrm{T}\bm{T}_i(x_i)=\sum_{i=1}^dA_i(\bm{\theta}_{i1})-\sum_{i=1}^dA_i(\bm{\theta}_{i0})+\ln\frac{1-\pi}{\pi}.
\end{equation}

\item[(2)] 
\begin{proof}
Suppose $P(X_i|Y=y)\sim \mathcal{N}(\mu_{iy}, \sigma_{i}^2)$, By solving $P(Y=0)P(\bm{X}|Y=0)=P(Y=1)P(\bm{X}|Y=1)$, we get
\begin{equation}
\pi\exp[-\frac{1}{2}\sum_{i=1}^d\frac{(x_i-\mu_{i1})^2}{\sigma_i^2}]=(1-\pi)\exp[-\frac{1}{2}\sum_{i=1}^d\frac{(x_i-\mu_{i0})^2}{\sigma_i^2}].
\end{equation}
After some simple math, we obtain the equation as
\begin{eqnarray}
\ln\frac{1-\pi}{\pi}&=&\sum_{i=1}^d\left(\frac{(x_i-\mu_{i0})^2}{2\sigma_i^2}-\frac{(x_i-\mu_{i1})^2}{2\sigma_i^2}\right)\nonumber\\
&=&\sum_{i=1}^d\left(\frac{\mu_{i1}-\mu_{i0}}{\sigma_i^2}x_i+\frac{\mu_{i0}^2-\mu_{i1}^2}{2\sigma_i^2}\right).
\end{eqnarray}
Therefore, the decision boundary
\begin{equation}
\sum_{i=1}^d\frac{\mu_{i1}-\mu_{i0}}{\sigma_i^2}x_i=\ln\frac{1-\pi}{\pi}-\sum_{i=1}^d\frac{\mu_{i0}^2-\mu_{i1}^2}{2\sigma_i^2}
\end{equation}
is linear to the sample space $\bm{X}=(x_1,x_2,\cdots,x_d)$.
\end{proof}
~\\
\end{solution}


\section{[70pts] Theoretical Analysis of $k$-means Algorithm}
给定样本集$\mathcal{D} = \{ \mathbf x_1,\mathbf x_2, \ldots, \mathbf x_n \}$, $k$-means聚类算法希望获得簇划分$\mathcal{C}=\{C_1,C_2,\cdots,C_k\}$, 使得最小化欧式距离
\begin{equation}
\label{eq-kmeans-l2}
J(\gamma, \mu_1,\ldots,\mu_k) = \sum_{i=1}^n \sum_{j=1}^k \gamma_{ij}||\mathbf x_i - \mu_j||^2
\end{equation} 
其中, $\mu_1, \ldots, \mu_k$为$k$个簇的中心(means), $\gamma \in \mathbb{R}^{n\times k}$为指示矩阵(indicator matrix)定义如下：若$\mathbf x_i$属于第$j$个簇, 则$\gamma_{ij} = 1$, 否则为0. 

则最经典的$k$-means聚类算法流程如算法\ref{algo:kmeans}中所示(与课本中描述稍有差别, 但实际上是等价的)。
\begin{algorithm}[]
\label{algo:kmeans}
\caption{$k$-means Algorithm}
\setcounter{AlgoLine}{0}
Initialize $\mu_1, \ldots, \mu_k$.\\
\Repeat{the objective function $J$ no longer changes}{
\textbf{Step 1}: Decide the class memberships of $\{\mathbf x_i\}_{i=1}^n$ by assigning each of them to its nearest cluster center.
\begin{align*}
\gamma_{ij} =
\begin{cases}
1,& ||\mathbf x_i - \mu_j||^2 \le ||\mathbf x_i - \mu_{j'}||^2, \forall j' \\
0, & \text{otherwise} 
\end{cases}
\end{align*}\\
\textbf{Step 2}: For each $j \in \{1, \cdots, k\}$, recompute $\mu_j$ using the updated $\gamma$ to be the center of mass of all points in $C_j$: 
\begin{align*}
\mu_j = \frac{\sum_{i=1}^n \gamma_{ij}\mathbf x_i}{\sum_{i=1}^n \gamma_{ij}}
\end{align*}
}
\end{algorithm}

\begin{enumerate}[(1)]

\item \textbf{[10pts]} 试证明, 在算法\ref{algo:kmeans}中, \textbf{Step 1}和\textbf{Step 2}都会使目标函数$J$的值降低.

\item \textbf{[10pts]} 试证明, 算法\ref{algo:kmeans}会在有限步内停止。

\item {\textbf{[10pts]} 试证明, 目标函数$J$的最小值是关于$k$的非增函数, 其中$k$是聚类簇的数目。}

\item {\textbf{[20pts]} 记$\hat{\mathbf{x}}$为$n$个样本的中心点, 定义如下变量,
\begin{table}[h]
\centering
\label{table:equation}
\begin{tabular}{ l | c }
  \hline			
total deviation & $T(X) = \sum_{i=1}^n \lVert \mathbf x_i - \hat{\mathbf x}\rVert^2/n$ \\
intra-cluster deviation & $W_j(X) = \sum_{i=1}^n \gamma_{ij} \lVert\mathbf x_i - \mu_j \rVert^2/\sum_{i=1}^n \gamma_{ij}$ \\
inter-cluster deviation & $B(X) = \sum_{j=1}^k \frac{ \sum_{i=1}^n \gamma_{ij}}{n}  \lVert\mu_j -\hat{\mathbf x} \rVert^2$\\
  \hline  
\end{tabular}
\end{table}

试探究以上三个变量之间有什么样的等式关系？基于此, 请证明, $k$-means聚类算法可以认为是在最小化intra-cluster deviation的加权平均, 同时近似最大化inter-cluster deviation.}

\item { \textbf{[20pts]} 在公式\eqref{eq-kmeans-l2}中, 我们使用$\ell_2$-范数来度量距离(即欧式距离), 下面我们考虑使用$\ell_1$-范数来度量距离

\begin{equation}
\label{eq-kmeans-l1}
J'(\gamma, \mu_1,\ldots,\mu_k) = \sum_{i=1}^n \sum_{j=1}^k \gamma_{ij}||\mathbf x_i - \mu_j||_1
\end{equation}

\begin{itemize} 
\item \textbf{[10pts]} 请仿效算法\ref{algo:kmeans}($k$-means-$\ell_2$算法), 给出新的算法(命名为$k$-means-$\ell_1$算法)以优化公式\ref{eq-kmeans-l1}中的目标函数$J'$.
\item \textbf{[10pts]} 当样本集中存在少量异常点(\href{https://en.wikipedia.org/wiki/Outlier}{outliers})时, 上述的$k$-means-$\ell_2$和$k$-means-$\ell_1$算法, 我们应该采用哪种算法？即, 哪个算法具有更好的鲁棒性？请说明理由。
\end{itemize}}

\end{enumerate}

\begin{solution}
\item[(1)]
\begin{proof}
In Step 1, a sample will be reassigned to another class if its previous nearest-class distance is not minimal. Specifically, for each sample $\bm{x}_i$, suppose its previous indicator vector was $\bm\gamma_i$ and is updated in Step 1 to be $\bm{\hat{\gamma_i}}$. By the update criteria, we have
\begin{equation}
\sum_{j=1}^k\hat\gamma_{ij}\|\bm{x}_i-\bm\mu_j\|^2=\sum_{\hat\gamma_{ij}=1}\|\bm{x}_i-\bm\mu_{j}\|^2\leq\sum_{\gamma_{ij}=1}\|\bm{x}_i-\bm\mu_j\|^2=\sum_{j=1}^k\gamma_{ij}\|\bm{x}_i-\bm\mu_j\|^2.
\end{equation}
Therefore, we get
\begin{equation}
\hat J=\sum_{i=1}^n \sum_{j=1}^k \hat \gamma_{ij}\|\bm x_i - \bm\mu_j\|^2\leq\sum_{i=1}^n \sum_{j=1}^k\gamma_{ij}\|\bm x_i - \bm\mu_j\|^2=J,
\end{equation}
so $J$ is decreasing after Step 1.

In Step 2, suppose the updated cluster centers are $\bm{\hat\mu}_j(j=1,2,\cdots, k)$, while the previous ones were $\bm\mu_j$s. We define function $f_j(\bm\mu)=\sum_{i=1}^n\gamma_{ij}\|\bm{x}_i-\bm\mu\|^2$, and by setting the derivative $\frac{df_j(\bm\mu)}{d\bm\mu}=\sum_{i=1}^n2\gamma_{ij}(\bm\mu-\bm{x}_i)$ as 0, we obtain
\begin{equation}
\sum_{i=1}^n(\gamma_{ij}\bm{\tilde\mu}-\gamma_{ij}\bm x_i)=0\quad\Rightarrow\quad \bm{\tilde\mu}\sum_{i=1}^n\gamma_{ij}=\sum_{i=1}^n\gamma_{ij}\bm x_i\quad \Rightarrow\quad \bm{\tilde\mu}=\frac{\sum_{i=1}^n\gamma_{ij}\bm x_i}{\sum_{i=1}^n\gamma_{ij}},
\end{equation}
which is exactly the expression in Step 2. Therefore, we have
\begin{equation}
\sum_{i=1}^n\gamma_{ij}\|\bm{x}_i-\bm{\hat\mu}_j\|^2=f_j(\bm{\hat\mu}_j)=\min_{\bm\mu}\{f_j(\bm\mu)\}\leq f_j(\bm\mu_j)=\sum_{i=1}^n\gamma_{ij}\|\bm{x}_i-\bm\mu_j\|^2, \quad \forall j\in\{1,2,\cdots,k\},
\end{equation}
\begin{equation}
\hat J=\sum_{j=1}^k\sum_{i=1}^n  \gamma_{ij}\|\bm x_i - \bm{\hat\mu_j}\|^2 \leq \sum_{j=1}^k\sum_{i=1}^n\gamma_{ij}\|\bm{x}_i-\bm\mu_j\|^2=J,
\end{equation}\
so $J$ is decreasing after Step 2.
\end{proof}

\item[(2)] 
\begin{proof}
In Step 2, we set all $\bm\mu_j$ according to $\gamma_{ij}$, so given $\bm\gamma$, we can determine the $J$ value (after some iteration) as follow:
\begin{equation}
J(\bm\gamma) = \sum_{i=1}^n\sum_{j=1}^k  \gamma_{ij}\|\bm x_i - \bm\mu_j\|^2= \sum_{i=1}^n\sum_{j=1}^k  \gamma_{ij}\left\|\bm x_i - \frac{\sum_{s=1}^n\gamma_{sj}\bm x_s}{\sum_{s=1}^n\gamma_{sj}}\right\|^2.
\end{equation}
Because $\bm\gamma$, as the indicator matrix, have a finite number (namely, $k^n$) of possible assignments, $J(\bm\gamma)$ also has a finite number ($\leq k^n$) of possible values. 

 Now, if we define $J_t$ as the $J$ value after the $t$th iteration, in Exercise (1) we have shown that $J_t$ is a non-increasing sequence. Hence, we can roughly run the $k$-means algorithm for $k^n+1$ iterations, and according to the Pigeonhole Principle, there must be at least two equal values in the sequence $J_1, J_2, \cdots, J_{k^n+1}$, and these two values must be consecutive since the sequence $J_t$ is non-increasing. Thus, the algorithm must terminate in at most $k^n+1$ iterations.
\end{proof}

\item[(3)]
\begin{proof}
We denote the minimum $J$ value when dividing samples into $k$ clusters as $J^{(k)}_{\text{min}}$. Here, we show that for all integer $k(k\geq 1)$, the inequality $J^{(k)}_{\text{min}}\geq J^{(k+1)}_{\text{min}}$ always holds.

For any $k\geq 1$, suppose when $J^{(k)}$ reaches minimum, the cluster division is $\mathcal C^{(k)}=\{C_1, C_2, \cdots, C_k\}$, and the minimum value is given by
\begin{equation}
J^{(k)}_{\text{min}}=\sum_{i=1}^n\sum_{j=1}^k \mathbb I(\bm{x}_i\in C_j)\cdot \|\bm{x}_i-\bm\mu_j\|^2.
\end{equation}
Now, we randomly pick one cluster (for instance $C_k$, without loss of generality) such that $|C_k|\geq 2$. Note that if such cluster doesn't exist, which indicates that each cluster only has one sample, a $(k+1)$-clustering simply doesn't make sense: a trivial case! Thus, we can split one sample $\bm{x}_p$ out of $C_k$ to be a new cluster $\hat C_{k+1}=\{\bm{x}_p\}$, so the new division $\hat{\mathcal C}^{(k+1)}=\{\hat C_1, \hat C_2, \cdots, \hat C_{k+1}\}$ is a $(k+1)$-clustering, in which
\begin{eqnarray}
&\hat C_i=C_i,\quad \forall i\in\{1,2,\cdots, k-1\},\\
&\hat C_k=C_k\backslash\{\bm{x}_p\}, \quad \hat C_{k+1}=\{\bm{x}_p\},\\
&\hat{\bm{\mu}}_i=\bm\mu_i,\quad \forall i\in\{1,2,\cdots, k\},\\
&\hat{\bm{\mu}}_{k+1}=\bm{x}_{p};
\end{eqnarray}
and its $J$ value satisfies
\begin{eqnarray}
\hat J^{(k+1)}&=&\sum_{i=1}^n\sum_{j=1}^{k+1} \mathbb I(\bm{x}_i\in \hat C_j)\cdot \|\bm{x}_i-\bm\mu_j\|^2\nonumber\\
&=&\sum_{\bm{x}_i\in \hat C_k} \|\bm{x}_i-\hat{\bm\mu}_k\|^2+\sum_{\bm{x}_i\in \hat C_{k+1}} \|\bm{x}_i-\hat{\bm\mu}_{k+1}\|^2+\sum_{j=1}^{k-1}\sum_{i=1}^n \mathbb I(\bm{x}_i\in \hat C_j)\cdot \|\bm{x}_i-\hat{\bm\mu}_j\|^2\nonumber\\
&=&\sum_{\bm{x}_i\in \hat C_k} \|\bm{x}_i-\bm\mu_k\|^2+\sum_{j=1}^{k-1}\sum_{i=1}^n \mathbb I(\bm{x}_i\in C_j)\cdot \|\bm{x}_i-{\bm\mu}_j\|^2\nonumber\\
&\leq&\sum_{i=1}^n\mathbb I(\bm{x}_i\in C_k)\|\bm{x}_i-\bm\mu_k\|^2+\sum_{j=1}^{k-1}\sum_{i=1}^n \mathbb I(\bm{x}_i\in C_j)\cdot \|\bm{x}_i-{\bm\mu}_j\|^2\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^{k} \mathbb I(\bm{x}_i\in C_j)\cdot \|\bm{x}_i-{\bm\mu}_j\|^2\nonumber\\
&=&J^{(k)}_{\text{min}}.
\end{eqnarray}
Since $\mathcal{\hat{C}}^{(k+1)}$ is just one way to separate samples into $(k+1)$ clusters, its $J$ value must be greater than the $(k+1)$-clustering minimum $J^{(k+1)}_{\text{min}}$. Therefore, $J^{(k)}_{\text{min}}\geq \hat J^{(k+1)}\geq J^{(k+1)}_{\text{min}}$, so the minimum value of $J$ is a non-increasing function of $k$.
\end{proof}
\item[(4)] 
\begin{equation}
nT(\bm{X})=\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+nB(\bm{X}).
\end{equation}
\begin{proof}
\allowdisplaybreaks
\begin{eqnarray}
nT(\bm{X})&=&\sum_{i=1}^n(\bm{x}_i-\hat{\bm{x}})^\mathrm{T}(\bm{x}_i-\hat{\bm{x}})=\sum_{i=1}^n(\bm{x}_i^\mathrm{T}\bm{x}_i-2\bm{\hat{x}}^\mathrm{T}\bm{x}_i+\hat{\bm{x}}^\mathrm{T}\hat{\bm x})\nonumber\\
&=&\sum_{i=1}^n\bm{x}_i^\mathrm{T}\bm{x}_i-2\bm{\hat{x}}^\mathrm{T}\sum_{i=1}^n\bm{x}_i+n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}=\sum_{i=1}^n\bm{x}_i^\mathrm{T}\bm{x}_i-n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}\bm{x}_i^\mathrm{T}\bm{x}_i-n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}\|\bm{x}_i-\bm\mu_j\|^2+\sum_{i=1}^n\sum_{j=1}^k2\gamma_{ij}\bm\mu_j^\mathrm{T}\bm{x}_i-\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}\bm\mu_j^\mathrm{T}\bm\mu_j-n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+\sum_{j=1}^k2\bm\mu_j^\mathrm{T}\sum_{i=1}^n\gamma_{ij}\bm{x}_i-\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}\bm\mu_j^\mathrm{T}\bm\mu_j-n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+\sum_{j=1}^k2\bm\mu_j^\mathrm{T}\sum_{i=1}^n\gamma_{ij}\bm\mu_j-\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}\bm\mu_j^\mathrm{T}\bm\mu_j-n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}\bm\mu_j^\mathrm{T}\bm\mu_j-n\hat{\bm{x}}^\mathrm{T}\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}(\bm\mu_j^\mathrm{T}\bm\mu_j+\hat{\bm{x}}^\mathrm{T}\hat{\bm x})-2\left(\sum_{i=1}^n\hat{\bm{x}}^\mathrm{T}\right)\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}(\bm\mu_j^\mathrm{T}\bm\mu_j+\hat{\bm{x}}^\mathrm{T}\hat{\bm x})-2\left(\sum_{j=1}^k\sum_{i=1}^n\gamma_{ij}\bm\mu_j^\mathrm{T}\right)\hat{\bm x}\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}(\bm\mu_j^\mathrm{T}\bm\mu_j-2\bm\mu_j^\mathrm{T}\hat{\bm x}+\hat{\bm{x}}^\mathrm{T}\hat{\bm x})\nonumber\\
&=&\sum_{i=1}^n\sum_{j=1}^k\gamma_{ij}W_j(\bm{X})+nB(\bm{X}).\nonumber\\
\end{eqnarray}
\end{proof}

In $k$-means, the objective function
\begin{eqnarray}
J(\bm\gamma) &=& \sum_{i=1}^n\sum_{j=1}^k  \gamma_{ij}\left\|\bm x_i - \frac{\sum_{s=1}^n\gamma_{sj}\bm x_s}{\sum_{s=1}^n\gamma_{sj}}\right\|^2=\sum_{j=1}^k\min_{\bm\mu_j}\sum_{i=1}^n\gamma_{ij}\|\bm x_i-\bm\mu_j\|^2=\sum_{j=1}^k\min_{\bm\mu_j}\sum_{i=1}^n\gamma_{ij}W_j(\bm X)\nonumber\\
&=&\min_{\bm\mu_1, \bm\mu_2,\cdots, \bm\mu_k}\sum_{j=1}^k\left(\sum_{i=1}^n\gamma_{ij}\right)W_j(\bm X)\nonumber\\
&=&\min_{\bm\mu_1, \bm\mu_2,\cdots, \bm\mu_k}(nT(\bm X)-nB(\bm X))=nT(\bm X)-n\max_{\bm\mu_1, \bm\mu_2,\cdots, \bm\mu_k}B(\bm X).
\end{eqnarray}
Therefore, the algorithm can be considered as minimizing the weighted average of $W_j(\bm X)$; since $T(\bm X)$ is constant, the algorithm is also maximizing $B(\bm X)$.

\item[(5)] The $k$-means-$\ell_1$ algorithm is shown in Algo.(\ref{k-means-l1}). The $\ell_1$ version is more robust for outliers, because when updating $\bm\mu$,  we take the median instead of mean value, making outliers negligible in operation.

\begin{algorithm}[hb]
\label{k-means-l1}
\caption{$k$-means-$\ell_1$ Algorithm}
\setcounter{AlgoLine}{0}
Initialize $\mu_1, \ldots, \mu_k$.\\
\Repeat{the objective function $J$ no longer changes}{
\textbf{Step 1}: Decide the class memberships of $\{\bm x_i\}_{i=1}^n$ by assigning each of them to its nearest cluster center.
\begin{align*}
\gamma_{ij} =
\begin{cases}
1,& \|\bm x_i - \bm\mu_j\|_1 \leq \|\bm x_i - \bm\mu_{j'}\|_1, \forall j' \\
0, & \text{otherwise} 
\end{cases}
\end{align*}\\
\textbf{Step 2}: For each $j \in \{1, \cdots, k\}$, recompute $\mu_j$ using the updated $\gamma$ to be the center of mass of all points in $C_j$: 
\begin{align*}
\mu_j = \text{med}(\{\bm x_i|\gamma_{ij}=1\}),
\end{align*}
in which $\{\bm x_i|\gamma_{ij}=1\}$ means the set of samples in cluster $C_j$, and $\text{med}(\cdot)$ means taking the median for each dimension.
}
\end{algorithm}
\end{solution}

\newpage
\section{[50pts] Kernel, Optimization and Learning}
给定样本集$\mathcal{D} = \{(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\cdots,(\mathbf{x}_m,y_m)\}$, $\mathcal{F} = \{\Phi_1 \cdots,\bm \Phi_d\}$为非线性映射族。考虑如下的优化问题
\begin{equation}
\label{eq-primal}
\min_{\mathbf w, \mu\in \Delta_q} \quad \frac{1}{2} \sum_{k=1}^d \frac{1}{\mu_k}\lVert\mathbf w_k\rVert_2^2 + C\sum_{i=1}^m \max\left\lbrace 0,1 - y_i\left(\sum_{k=1}^d \mathbf w_k \cdot \bm \Phi_k(\mathbf{x}_i)\right) \right\rbrace
\end{equation}
其中, $\Delta_q = \left\lbrace \bm{ \mu} | \mu_k\geq 0, k=1,\cdots,d; \lVert \bm{ \mu} \rVert_q = 1\right\rbrace$.

\begin{enumerate}[(1)]
\item{ \textbf{[30pts]} 请证明, 下面的问题\ref{eq-dual}是优化问题\ref{eq-primal}的对偶问题。
\begin{equation}
\label{eq-dual}
	\begin{split}
\max_{\bm \alpha} &\quad 2\bm \alpha^\mathrm T \mathbf{1}- \left\lVert
 \begin{matrix}
   \bm \alpha^\mathrm{T}\mathbf Y^\mathrm{T} \mathbf K_1 \mathbf Y  \bm \alpha \\
   \vdots \\
  \bm \alpha^\mathrm{T}\mathbf Y^\mathrm{T} \mathbf K_d \mathbf Y  \bm \alpha 
  \end{matrix}
  \right\rVert_p\\
  \text{s.t.} &\quad  \mathbf{0} \leq \bm \alpha  \leq \mathbf{C} 
  \end{split}
\end{equation}
其中, $p$和$q$满足共轭关系, 即$\frac{1}{p}+\frac{1}{q}=1$. 同时, $\mathbf Y = \text{diag}([y_1,\cdots,y_m])$, $\mathbf K_k$是由$\bm \Phi_k$定义的核函数(kernel).}
\item{ \textbf{[20pts]} 考虑在优化问题\ref{eq-dual}中, 当$p=1$时, 试化简该问题。}
\end{enumerate}

\begin{solution}
\item[(1)] 
\begin{proof}
Define the slack variables $\xi_i\geq 0(i=1,\cdots m)$, so the problem is equivalent as
\begin{equation}
\begin{split}
\min_{\bm{W}, \bm{\mu}, \bm{\xi}} &\quad\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k+C\sum_{i=1}^m\xi_i,\\
\text{s.t.} &\quad \sum_{k=1}^d\mu_k^q=1; \quad \mu_k\geq0,\quad k=1,2,\cdots,d;\\
&\quad y_i\left(\sum_{k=1}^d\bm{w}_k^\mathrm{T}\cdot\bm{\Phi}_k(\bm{x_i})\right)\geq 1-\xi_i, \quad i=1,2,\cdots m;\\
&\quad \xi_i\geq 0, \quad i=1,2,\cdots m.
\end{split}\label{pr}
\end{equation}
The Lagrange function is given by
\begin{eqnarray}\label{lag}
L(\bm{W}, \bm{\mu}, \bm{\xi}, \lambda, \bm{\alpha}, \bm{\beta}, \bm{\gamma})&=&\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k+C\sum_{i=1}^m\xi_i+\lambda(\sum_{k=1}^d\mu_k^q-1)+\nonumber\\
&+&\sum_{i=1}^m\alpha_i[1-\xi_i-y_i(\sum_{k=1}^d\bm{w}_k^\mathrm{T}\cdot\bm{\Phi}_k(\bm{x_i}))]+\sum_{i=1}^m\beta_i(-\xi_i)+\sum_{k=1}\gamma_k(-\mu_k),\nonumber\\
\end{eqnarray}
where $\lambda$, $\bm{\alpha}$, $\bm{\beta}$ are Lagrange multipliers and satisfy $\alpha_i\geq0$, $\beta_i\geq0$ $(i=1,2,\cdots,m)$. Setting the partial derivatives $\frac{\partial L}{\partial \bm{w}_k}$, $\frac{\partial L}{\partial\mu_k}$ and $\frac{\partial L}{\partial\xi_i}$ to 0, we get
\begin{eqnarray}
\frac{\partial L}{\partial\bm{w}_k}&=&\frac{1}{\mu_k}\bm{w}_k-\sum_{i=1}^m\alpha_i y_i\bm{\Phi}_k(\bm{x_i})=0,\\
\frac{\partial L}{\partial\mu_k}&=&-\frac{1}{2\mu_k^2}\bm{w}_k^\mathrm{T}\bm{w}_k+\lambda q\mu_k^{q-1}-\gamma_k=0,\\
\frac{\partial L}{\partial\xi_i}&=&C-\alpha_i-\beta_i=0,
\end{eqnarray}
or equivalently,
\begin{eqnarray}
\bm{w}_k=\mu_k\sum_{i=1}^m\alpha_i y_i\bm{\Phi}_k(\bm{x_i}), &\quad \forall k\in\{1,2,\cdots, d\};\label{wk}\\
\bm{w}_k^\mathrm{T}\bm{w}_k=2\lambda q\mu_k^{q+1}-2\mu_k^2\gamma_k, &\quad\forall k\in\{1,2,\cdots, d\};\label{wk2}\\
\alpha_i+\beta_i=C, &\quad\forall i \in\{1,2,\cdots, m\}.\label{cab}
\end{eqnarray}

For the sake of simplicity, we define the variables $M_k(k=1,2,\cdots, d)$ as
\begin{equation}\label{mkdef}
M_k = \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\bm{\Phi}_k^\mathrm{T}(\bm{x}_i)\bm{\Phi}_k(\bm{x}_j).
\end{equation}
It is easy to prove that $M_k=\bm\alpha^\mathrm{T}\bm Y^\mathrm{T} \bm K_k \bm Y  \bm \alpha$. Specifically, by its definition Eq.(\ref{mkdef}),
\begin{eqnarray}
M_k &=& \left(\alpha_1 y_1, \cdots, \alpha_m y_m\right)\left(
\begin{array}{cccc}
\bm{\Phi}_k^\mathrm{T}(\bm{x}_1)\bm{\Phi}_k(\bm{x}_1)	& \bm{\Phi}_k^\mathrm{T}(\bm{x}_1)\bm{\Phi}_k(\bm{x}_2)	& \cdots	& \bm{\Phi}_k^\mathrm{T}(\bm{x}_1)\bm{\Phi}_k(\bm{x}_m)\\
\bm{\Phi}_k^\mathrm{T}(\bm{x}_2)\bm{\Phi}_k(\bm{x}_1)	& \bm{\Phi}_k^\mathrm{T}(\bm{x}_2)\bm{\Phi}_k(\bm{x}_2)	& \cdots	& \bm{\Phi}_k^\mathrm{T}(\bm{x}_2)\bm{\Phi}_k(\bm{x}_m)\\
\vdots	& \vdots	& \ddots	& \vdots\\
\bm{\Phi}_k^\mathrm{T}(\bm{x}_m)\bm{\Phi}_k(\bm{x}_1)	& \bm{\Phi}_k^\mathrm{T}(\bm{x}_m)\bm{\Phi}_k(\bm{x}_2)	& \cdots	& \bm{\Phi}_k^\mathrm{T}(\bm{x}_m)\bm{\Phi}_k(\bm{x}_m)\\
\end{array}
\right)\left(
\begin{array}{c}
\alpha_1 y_1\\
\alpha_2 y_2\\
\vdots\\
\alpha_m y_m
\end{array}
\right)\nonumber\\
&=&\left(\alpha_1, \alpha_2, \cdots, \alpha_m\right)\left(\
\begin{array}{cccc}
y_1	& 0	& \cdots	& 0\\
0	& y_2	& \cdots	& 0\\
\vdots	& \vdots	& \ddots	& \vdots\\
0	& 0	& \cdots	& y_m
\end{array}
\right)\bm{K}_k\left(\
\begin{array}{cccc}
y_1	& 0	& \cdots	& 0\\
0	& y_2	& \cdots	& 0\\
\vdots	& \vdots	& \ddots	& \vdots\\
0	& 0	& \cdots	& y_m
\end{array}
\right)\left(
\begin{array}{c}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_m
\end{array}
\right)\nonumber\\
&=& \bm{\alpha}^\mathrm{T}\bm{Y}^\mathrm{T}\bm{K}_k\bm{Y}\bm{\alpha},
\end{eqnarray}
where $\bm{Y}=\text{diag}(y_1,\cdots, y_m)$ and $\bm{K}_k$ is the kernel matrix of mapping $\bm{\Phi}_k(\cdot)$. 

Now, we will derive the dual problem. The Lagrange function Eq.(\ref{lag}) could be rewritten as
\begin{eqnarray}
L(\bm{\alpha})&=&\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k+\sum_{i=1}^m(C-\alpha_i-\beta_i)\xi_i+\lambda(\sum_{k=1}^d\mu_k^q-1)-\sum_{k=1}^d\gamma_k\mu_k\nonumber\\
&&+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy_i(\sum_{k=1}^d\bm{w}_k^\mathrm{T}\cdot\bm{\Phi}_k(\bm{x_i}))-\sum_{k=1}^d\gamma_k\mu_k\nonumber\\
&=&\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy_i(\sum_{k=1}^d\bm{w}_k^\mathrm{T}\cdot\bm{\Phi}_k(\bm{x_i}))-\sum_{k=1}^d\gamma_k\mu_k\quad\text{(\textit{by Eq.}(\ref{cab}))}\nonumber\\
&=&\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k+\sum_{i=1}^m\alpha_i-\sum_{k=1}^d\bm{w}_k^\mathrm{T}\sum_{i=1}^m\alpha_iy_i\bm{\Phi}_k(\bm{x_i})-\sum_{k=1}^d\gamma_k\mu_k\nonumber\\
&=&\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k+\sum_{i=1}^m\alpha_i-\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k-\sum_{k=1}^d\gamma_k\mu_k\quad\quad\quad\quad\quad\quad\text{\quad(\textit{by Eq.}(\ref{wk}))}\nonumber\\
&=&\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{k=1}^d\frac{1}{\mu_k}\bm{w}_k^\mathrm{T}\bm{w}_k-\sum_{k=1}^d\gamma_k\mu_k\nonumber\\
&=&\sum_{i=1}^m\alpha_i-\sum_{k=1}^d\lambda q\mu_k^{q}\quad\quad\quad\quad\quad\quad\quad\quad\ \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\text{\quad(\textit{by Eq.}(\ref{wk2}))}\nonumber\\
&=&\sum_{i=1}^m\alpha_i-\lambda q.
\end{eqnarray}

In fact, it is easy to see that $\mu_k$ can never equal 0, because otherwise, the $\frac{1}{\mu_k}$ term in Eq.(\ref{eq-primal}) does not make sense (making the objective function value go to $+\infty$). Thus, the optimal solution could never lie on the boundary of constraint $\mu_k\geq 0$. According to the KKT condition $\gamma_k\mu_k=0$, we have $\gamma_k=0$ holds for all $k\in\{1,2,\cdots, d\}$.

Now, by plugging Eq.(\ref{wk}) into Eq.(\ref{wk2}), we get
\begin{eqnarray}
\bm{w}_k^\mathrm{T}\bm{w}_k&=&\left(\mu_k\sum_{i=1}^m\alpha_i y_i\bm{\Phi}_k(\bm{x_i})\right)^\mathrm{T}\left(\mu_k\sum_{j=1}^m\alpha_j y_j\bm{\Phi}_k(\bm{x_j})\right)=\mu_k^2\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\bm{\Phi}_k^\mathrm{T}(\bm{x}_i)\bm{\Phi}_k(\bm{x}_j)\nonumber\\
&=&\mu_k^2M_k=2\lambda q\mu_k^{q+1}-2\mu_k^2\gamma_k.\label{mk2lambda}
\end{eqnarray}
So, we have $M_k=2\lambda q\mu_k^{q-1}-2\gamma_k=2\lambda q\mu_k^{q-1}$, and therefore, for the conjugate number $p=\frac{q}{q-1}$, we have
\begin{equation}
\sum_{k=1}^dM_k^p=\sum_{k=1}^d(2\lambda q)^p\mu_k^q=(2\lambda q)^p\sum_{k=1}^d\mu_k^q=(2\lambda q)^p.
\end{equation}
This just indicates that
\begin{equation}
2\lambda q=\left(\sum_{k=1}^dM_k^p\right)^{\frac{1}{p}}
=\left\lVert
\begin{matrix}
  M_1 \\
   \vdots \\
  M_d
  \end{matrix}\right\rVert_p
=\left\lVert
\begin{matrix}
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_1 \bm{Y}\bm\alpha \\
   \vdots \\
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_d \bm{Y}\bm\alpha \\
  \end{matrix}\right\rVert_p.\label{mvp}
\end{equation}


Thus, according to Eq.(\ref{mvp}), the dual problem's objective function could be
\begin{equation}
\Gamma(\bm\alpha)=2L(\bm{\alpha})=2\sum_{i=1}^m\alpha_i-2\lambda q=2\bm\alpha^\mathrm T \bm{1}- \left\lVert
\begin{matrix}
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_1 \bm{Y}\bm\alpha \\
   \vdots \\
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_d \bm{Y}\bm\alpha \\
  \end{matrix}\right\rVert_p.
\end{equation}
The corresponding KKT conditions are:  for $\forall i \in\{1,2,\cdots, m\}$, $\forall k \in\{1,2,\cdots, d\}$
\begin{equation}
\left\{
\begin{array}{l}
\alpha_i\geq 0, \quad \beta_i\geq 0,\\
y_i(\sum_{k=1}^d\bm{w}_k\cdot\bm{\Phi}_k(\bm{x_i}))\geq 1-\xi_i,\\
\alpha_i(y_i(\sum_{k=1}^d\bm{w}_k\cdot\bm{\Phi}_k(\bm{x_i}))-1+\xi_i)=0,\\
\xi_i\geq 0, \quad \beta_i\xi_i=0,\\
\mu_k\geq 0, \quad \gamma_k\mu_k=0.
\end{array}
\right.
\end{equation}
Since $\alpha_i+\beta_i=C$, $\alpha_i\geq 0$, $\beta_i\geq 0$, we conclude that $\bm{0}\leq\bm\alpha\leq\bm{C}$. Consequently, we achieve the dual problem as
\begin{equation}
\begin{split}
\max_{\bm{\alpha}} &\quad 2\bm\alpha^\mathrm T \bm{1}- \left\lVert
\begin{matrix}
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_1 \bm{Y}\bm\alpha \\
   \vdots \\
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_d \bm{Y}\bm\alpha \\
  \end{matrix}\right\rVert_p,\\
\text{s.t.} &\quad \bm{0}\leq\bm\alpha\leq\bm{C}.
\end{split}
\end{equation}
\end{proof}

\item[(2)] Taking $p=1$, we claim that the optimization problem would transform into a classic kernelized soft margin SVM (and without intercept term). 

Recall that the kernel matrices $\bm{K}_k$ are positive semi-definite, so $ \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_k \bm{Y}\bm\alpha\geq 0$. We obtain the objective function, given $p=1$, in Eq.(\ref{eq-dual}) as
\begin{eqnarray}
\Gamma(\bm\alpha)&=&2\bm\alpha^\mathrm T \bm{1}- \left\lVert
\begin{matrix}
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_1 \bm{Y}\bm\alpha \\
   \vdots \\
   \bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_d \bm{Y}\bm\alpha \\
  \end{matrix}\right\rVert_1\nonumber\\
 &=&2\bm\alpha^\mathrm T \bm{1}-\sum_{k=1}^d\left|\bm\alpha^\mathrm{T}\bm{Y}^\mathrm{T} \bm{K}_k \bm{Y}\bm\alpha\right|\nonumber\\
  &=&2\sum_{i=1}^m\alpha_i-\sum_{k=1}^d\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\bm{\Phi}_k^\mathrm{T}(\bm{x}_i)\bm{\Phi}_k(\bm{x}_j)\nonumber\\
&=&2\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\sum_{k=1}^d\bm{K}_k(\bm{x}_i,\bm{x}_j).
\end{eqnarray}
Therefore, if we define the linear combination $\bm{K}(\bm{x}_i,\bm{x}_j)=\sum_{k=1}^d\bm{K}_k(\bm{x}_i,\bm{x}_j)$, which is also a valid kernel function, the dual problem Eq.(\ref{eq-dual}) is equivalent as
\begin{equation}
\begin{split}
\max_{\bm{\alpha}} &\quad \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\bm{K}(\bm{x}_i,\bm{x}_j),
\\
\text{s.t.} &\quad \bm{0}\leq\bm\alpha\leq\bm{C}.
\end{split}\label{smsvmgai}
\end{equation}
Notice that Eq.(\ref{smsvmgai}) is exactly the same as the dual problem of the kernelized soft margin SVM, except that the intercept term $b$ in the boundary expression $\bm{w}^\mathrm{T}\bm{x}+b$ is dropped (so one constraint is deleted). ln conclusion, setting $p=1$, the dual problem would decay into a classic kernelized soft margin SVM (without intercept term). 

\begin{Remark}
In fact, if we take the conjugate of 1 as $\infty$, and start from the primal problem's prospective, we can get the same consistent result.

Given $p=1$, $\frac{1}{p}+\frac{1}{q}=1$, we get $q=\infty$, so the primal problem constrains that $\|\bm\mu\|_\infty=1$, i.e. $\max_{k}\{\mu_k\}=1$. Thus, the objective function in Eq.(\ref{eq-primal}) satisfies
\begin{eqnarray}
f(\bm{W},\bm{\mu})&=&\frac{1}{2} \sum_{k=1}^d \frac{1}{\mu_k}\lVert\bm w_k\rVert_2^2 + C\sum_{i=1}^m \max\left\lbrace 0,1 - y_i\left(\sum_{k=1}^d \bm w_k \cdot \bm \Phi_k(\bm{x}_i)\right) \right\rbrace\nonumber\\
&\geq&\frac{1}{2} \sum_{k=1}^d\lVert\bm w_k\rVert_2^2 + C\sum_{i=1}^m \max\left\lbrace 0,1 - y_i\left(\sum_{k=1}^d \bm w_k \cdot \bm \Phi_k(\bm{x}_i)\right)\right\rbrace\nonumber\\
&=&f(\bm{W}, \bm{1}).
\end{eqnarray}
Therefore, the function $f(\bm{W},\bm{\mu})$ must reach its minimum at $\bm{\mu}=\bm{1}$. Plug $\bm\mu=\bm1$ into Eq.(\ref{pr}), we get the primal problem as
\begin{equation}
\begin{split}
\min_{\bm{W}, \bm{\xi}} &\quad\frac{1}{2}\sum_{k=1}^d\bm{w}_k^\mathrm{T}\bm{w}_k+C\sum_{i=1}^m\xi_i,\\
\text{s.t.} &\quad y_i\left(\sum_{k=1}^d\bm{w}_k^\mathrm{T}\cdot\bm{\Phi}_k(\bm{x_i})\right)\geq 1-\xi_i, \quad i=1,2,\cdots m;\\
&\quad \xi_i\geq 0, \quad i=1,2,\cdots m,
\end{split}
\end{equation}
which is obviously the classic form of a kernelized soft margin SVM (without intercept term $b$).
\end{Remark}
~\\
\end{solution}


\begin{thebibliography}{99}
\bibitem{ref: conj_dist} Wikipedia contributors. "Exponential family." \text{Wikipedia, The Free Encyclopedia}, 16 May. 2017. Available at: \url{https://en.wikipedia.org/wiki/Exponential_family#Bayesian_estimation:_conjugate_distributions} [Accessed 11 Jun. 2017]
\end{thebibliography}

\end{document}