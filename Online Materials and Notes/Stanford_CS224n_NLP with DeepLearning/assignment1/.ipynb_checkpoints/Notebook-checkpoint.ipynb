{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Ipython Notebook, I can write down the mathmatical expression with latex, which allows me to understand my codes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q_3 word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\" \n",
    "    Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "#    print (x.sum(axis=1).reshape(-1,1))\n",
    "    x = x/np.sqrt((x**2).sum(axis=1)).reshape(-1,1)\n",
    "#   Equivalent Form:\n",
    "    '''\n",
    "    x = x/np.sqrt((x**2).sum(axis-=1, keepdims = True))\n",
    "\n",
    "    '''\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[0.6        0.8       ]\n",
      " [0.4472136  0.89442719]]\n",
      "test passed\n"
     ]
    }
   ],
   "source": [
    "def test_normalize_rows():\n",
    "    print (\"Testing normalizeRows...\")\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]]))\n",
    "    print (x)\n",
    "    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print (\"test passed\")\n",
    "test_normalize_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the input arguments of the softmaxCostAndGradient function\n",
    "- ($\\hat{y}$) = predicted\n",
    "- ($\\hat{y} - y$) = (predicted[target] -= 1.)\n",
    "- cost = -log(prob)\n",
    "- gradPred = $\\frac{\\partial CE(y, \\hat{y})}{\\partial \\theta}$ = $U (\\hat{y} - y)$ = np.dot(prob, $\\hat{y} - y$)\n",
    "- grad = $\\frac{\\partial CE(y, \\hat{y})}{\\partial u_w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector \n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "            what is the meaning of the output vectors?\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#The math expression of the loss function can be found in the slides,\n",
    "# to get a better understanding, I will use the same notation same as the paper assignment\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # y has the same shape with y_hat but all zero values, \n",
    "    # whereas the target place has a value of 1.\n",
    "\n",
    "    #然后按照slides上的表达式 就直接求出cost\n",
    "    prob = softmax(np.dot(predicted, outputVectors.T))\n",
    "    cost = -np.log(prob[target])\n",
    "\n",
    "    #这一步是用来求出 y_hat - y\n",
    "    prob[target] -= 1.\n",
    "\n",
    "    #跟推导的结果一致，\n",
    "    gradPred = np.dot(prob, outputVectors)\n",
    "    \n",
    "    #这里我不是很清楚为什么要这么来写,这三种表达方式等价，我用的是我比较熟悉的一种\n",
    "    #grad = prob[:, np.newaxis] * predicted[np.newaxis, :]\n",
    "    #grad = np.outer(prob, predicted)\n",
    "    grad = np.dot(prob.reshape(-1,1), predicted.reshape(1, -1))\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'np.out(a,b)' is to combine the a(M, ) and b(N, ) into (M, N) array, where out[i][j] = a[i] * b[j]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "    indices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part is designed to execute the part(c) of the assignment problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J_{loss}$ = $-log(\\sigma(u_O^T v_C)) - \\Sigma_{k=1}^K log(\\sigma(-u_k^T v_C))$\n",
    "\n",
    "$\\frac{\\partial J_{loss}}{\\partial v_c}$ = $(\\sigma(u_O^T v_C)-1)u_O - \\Sigma_{k=1}^K (\\sigma(-u_k^T v_C)-1)u_k$\n",
    "\n",
    "$\\frac{\\partial J_{loss}}{\\partial u_O}$ = $[\\sigma(u_O^T v_C) - 1]v_C$\n",
    "\n",
    "$\\frac{\\partial J_{loss}}{\\partial u_k}$ = $-[\\sigma(-u_k^T v_C) - 1]v_C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "   \n",
    "    ### YOUR CODE HERE\n",
    "    ### these parameters can be derived directly from my paper assignment\n",
    "    prob =  np.dot(outputVectors, predicted)\n",
    "    cost = -np.log(sigmoid(prob[target])) - np.log(sigmoid(-prob[indices[1:]])).sum()\n",
    "\n",
    "\n",
    "    opp_sig = (sigmoid(-prob[indices[1:]]) - 1)\n",
    "    gradPred = (sigmoid(prob[target]) - 1) * outputVectors[target] \\\n",
    "            +  sum(opp_sig[:, np.newaxis] * outputVectors[indices[1:]])\n",
    "    \n",
    "    grad = np.zeros_like(outputVectors)\n",
    "    grad[target] = (sigmoid(prob[target]) - 1) * predicted\n",
    "\n",
    "    for k in indices[1:]:\n",
    "        grad[k] += (1.0 - sigmoid(-np.dot(outputVectors[k], predicted))) * predicted\n",
    "\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
